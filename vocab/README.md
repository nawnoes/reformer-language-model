## ğŸ“˜ Vocab & Tokenizer
`Sentencepiece`ì™€ `Wordpiece` ì¤‘ ê¸°ì¡´ì— ì‚¬ìš©í•´ë³´ì§€ ì•Šì€ Wordpiece Tokenizerë¥¼ ì´ìš©í•œ Tokenizer ì‚¬ìš©. ë¡œ
ì„¼í…ìŠ¤í”¼ìŠ¤ì™€ ì›Œë“œí”¼ìŠ¤ ëª¨ë‘ Subword Tokenizerì¤‘ í•˜ë‚˜ì¸ **BPE(Byte Pair Encoding)** ì— ì†í•œë‹¤. ì„¼í…ìŠ¤í”¼ìŠ¤ì˜ ê²½ìš° ë¹ˆë„ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì›Œë“œí”¼ìŠ¤ì˜ ê²½ìš° likelihoodë¥¼ ê¸°ë°˜ìœ¼ë¡œ BPEì„ ìˆ˜í–‰í•œë‹¤. 
#### WordPiece Tokenizer
1. huggingface tokenizers ì„¤ì¹˜
```shell script
pip install tokenizers
```
2. ë§ë­‰ì¹˜ íŒŒì¼ë¡œ Wordpiece vocab ìƒì„±
```python
import argparse
from tokenizers import BertWordPieceTokenizer

parser = argparse.ArgumentParser()

parser.add_argument("--corpus_file", type=str)
parser.add_argument("--vocab_size", type=int, default=22000) # ë§Œë“¤ Vocabì˜ ìˆ«ì 
parser.add_argument("--limit_alphabet", type=int, default=6000)

args = parser.parse_args()

tokenizer = BertWordPieceTokenizer(
    vocab_file=None,
    clean_text=True,
    handle_chinese_chars=True,
    strip_accents=False, # Must be False if cased model
    lowercase=False,
    wordpieces_prefix="##"
)

tokenizer.train(
    files=[args.corpus_file],
    limit_alphabet=args.limit_alphabet,
    vocab_size=args.vocab_size
)

tokenizer.save("./ch-{}-wpm-{}-pretty".format(args.limit_alphabet, args.vocab_size),True)
```
3. ìƒì„±ëœ vocab íŒŒì¼ ì „ì²˜ë¦¬
```python
import json # import json module

vocab_path = "../vocab/ch-6000-wpm-22000-pretty"

vocab_file = '../data/wpm-vocab-all.txt'
f = open(vocab_file,'w',encoding='utf-8')
with open(vocab_path) as json_file:
    json_data = json.load(json_file)
    for item in json_data["model"]["vocab"].keys():
        f.write(item+'\n')

    f.close()
```
4. Tokenizer í…ŒìŠ¤íŠ¸
```python
from transformers.tokenization_bert import BertTokenizer

vocab_path = "../data/wpm-vocab-all.txt"

tokenizer = BertTokenizer(vocab_file=vocab_path, do_lower_case=False)

test_str = ' [CLS] ë‚˜ëŠ” ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ë¥¼ ì¨ìš”. ì„±ëŠ¥ì´ ì¢‹ì€ì§€ í…ŒìŠ¤íŠ¸ í•´ë³´ë ¤ í•©ë‹ˆë‹¤. [SEP]'
print('í…ŒìŠ¤íŠ¸ ë¬¸ì¥: ',test_str)

encoded_str = tokenizer.encode(test_str,add_special_tokens=False)
print('ë¬¸ì¥ ì¸ì½”ë”©: ',encoded_str)

decoded_str = tokenizer.decode(encoded_str)
print('ë¬¸ì¥ ë””ì½”ë”©: ',decoded_str)

"""
í…ŒìŠ¤íŠ¸ ë¬¸ì¥:   [CLS] ë‚˜ëŠ” ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ë¥¼ ì¨ìš”. ì„±ëŠ¥ì´ ì¢‹ì€ì§€ í…ŒìŠ¤íŠ¸ í•´ë³´ë ¤ í•©ë‹ˆë‹¤. [SEP]
ë¬¸ì¥ ì¸ì½”ë”©:  [2, 9310, 4868, 6071, 12467, 21732, 12200, 6126, 6014, 4689, 6100, 18, 11612, 6037, 9389, 6073, 16784, 17316, 6070, 10316, 18, 3]
ë¬¸ì¥ ë””ì½”ë”©:  [CLS] ë‚˜ëŠ” ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ë¥¼ ì¨ìš”. ì„±ëŠ¥ì´ ì¢‹ì€ì§€ í…ŒìŠ¤íŠ¸ í•´ë³´ë ¤ í•©ë‹ˆë‹¤. [SEP]
"""
```